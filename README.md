# IMPROVING PRECISION OF AUTOMATED IMAGE CAPTIONING AND CONVERSION TO AUDIO FOR THE VISUALLY IMPAIRED

Image captioning systems for the blind provide an alternative way to describe the visual content of an image for those who are unable to see it. This allows them to better understand and experience the image, providing them with information and context that they would otherwise miss. The use of natural language captions makes this information more accessible and easier to understand, allowing blind individuals to participate more fully in various activities and experiences that involve visual media. This study aims to provide a holistic view of the process where relevant captions are generated, then being translated into the language that the user is most comfortable with and finally provided as an audio output. This helps add to the overall richness of the experience for the visually impaired that can help them participate actively in day-to-day tasks that they may have been deprived of due to a number of constraints. Additionally, for research purposes, it provides the opportunity to tune the system in a manner that most captions generated in several languages provide the quality or relevance that is required to make a project like this feasible in real life.
